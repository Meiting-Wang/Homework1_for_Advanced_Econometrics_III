%xelatex 或 pdflatex 编译
%导言区
\documentclass[UTF8]{article} %UTF8编码
\input{preface.tex}

%标题页设置
\title{
	Homework 1 for Advanced Econometrics III
}
\author{Meiting Wang\thanks{Meiting Wang, Student ID: 202020111002, Email: wangmeiting92@gmail.com}}
\date{\today}




%正文区
\begin{document}
\maketitle

\begin{enumerate}
    %problem boundary--------------------------------------------------------------------
    \item[5.1] In this problem you are to establish the algebraic equivalence between 2SLS and OLS estimation of an equation containing an additional regressor. Although the result is completely general, for simplicity consider a model with a single (suspected) endogenous variable: 
    \begin{gather*}
        y_{1}=\mathbf{z}_{1} \boldsymbol{\delta}_{1}+\alpha_{1} y_{2}+u_{1} \\
        y_{2}=\mathbf{z} \pi_{2}+v_{2}
    \end{gather*}
    For notational clarity, we use $y_{2}$ as the suspected endogenous variable and $\mathbf{z}$ as the vector of all exogenous variables. The second equation is the reduced form for $y_{2}$. Assume that $z$ has at least one more element than $\mathbf{z}_{1}$. We know that one estimator of $\left(\delta_{1}, \alpha_{1}\right)$ is the $2 \mathrm{SLS}$ estimator using instruments $\mathbf{X}$. Consider an alternative estimator of $\left(\delta_{1}, \alpha_{1}\right):$ (a) estimate the reduced form by OLS, and save the residuals $\hat{v}_{2} ;(\mathrm{b})$ estimate the following equation by OLS:
    \begin{gather}
    y_{1}=\mathbf{z}_{1} \delta_{1}+\alpha_{1} y_{2}+\rho_{1} \hat{v}_{2}+error \tag{5.52} \label{eq:5.1-1}
    \end{gather}
    Show that the OLS estimates of $\boldsymbol{\delta}_{1}$ and $\alpha_{1}$ from this regression are identical to the 2SLS estimators. (Hint: Use the partitioned regression algebra of OLS. In particular, if $\hat{y}=\mathbf{x}_{1} \hat{\boldsymbol{\beta}}_{1}+\mathbf{x}_{2} \hat{\boldsymbol{\beta}}_{2}$ is an OLS regression, $\hat{\boldsymbol{\beta}}_{1}$ can be obtained by first regressing $\mathbf{x}_{1}$ on $\mathbf{x}_{2},$ getting the residuals, say $\ddot{\mathbf{x}}_{1},$ and then regressing $y$ on $\ddot{\mathbf{x}}_{1} ;$ see, for example, Davidson and MacKinnon (1993, Section 1.4). You must also use the fact that $\mathbf{z}_{1}$ and $\hat{v}_{2}$ are orthogonal in the sample.)
    
    \textbf{Answer:} To obtain the OLS estimators of the equation \eqref{eq:5.1-1}, as the hint shows, we can do the following alternative things: 
    \begin{itemize}
        \item Get the residuals of the regression $\mathbf{z}_1$ on $\hat{v}_2$. Due to the orthogonality between $\mathbf{z}_1$ and $\hat{v}_2$, the residual will be $\mathbf{z}_1 - \mathbf{cons}$.
        \item Get the residuals of the regression $y_2$ on $\hat{v}_2$. From the regression $y_2$ on $\mathbf{z}$, we can write $y_2 = \hat{y}_2 + \hat{v}_2$, where the $\hat{y}_2$ are the fitted values and the $\hat{v}_2$ are the residuals. It is easy to know that $\hat{y}_2$ is orthogonal with $\hat{v}_2$ from the nature of OLS, so the residuals from the regression $y_2$ on $\hat{v}_2$ will be $\hat{y}_2 - cons$.
        \item Do regression $y_1$ on the residuals obtained above.
    \end{itemize}
    According to the alternative steps to get the OLS estimators of the equation \eqref{eq:5.1-1}, we know that the OLS estimators are identical to the 2SLS estimators showed in the question.
    
    
    %problem boundary--------------------------------------------------------------------
    \item[5.7] Consider model (5.45) where $v$ has zero mean and is uncorrelated with $x_{1}, \cdots, x_{K}$ and $q .$ The unobservable $q$ is thought to be correlated with at least some of the $x_{j} .$ Assume without loss of generality that $\mathrm{E}(q)=0$. You have a single indicator of $q,$ written as $q_{1}=\delta_{1} q+a_{1}, \delta_{1} \neq 0,$ where $a_{1}$ has zero mean and is uncorrelated with each of $x_{j}, q,$ and $v .$ In addition, $z_{1}, z_{2}, \cdots, z_{M}$ is a set of variables that are (1) redundant in the structural equation (5.45) and (2) uncorrelated with $a_{1}$.
    \begin{enumerate}
        \item Suggest an IV method for consistently estimating the $\beta_{j} .$ Be sure to discuss what is needed for identification.
        
        \textbf{Answer:} Recall the equation \eqref{eq:5.7-1}
        \begin{gather}
            y=\beta_{0}+\beta_{1} x_{1}+\cdots+\beta_{K} x_{K}+\gamma q+v \tag{5.45} \label{eq:5.7-1}
        \end{gather}
        Plugging $q_{1}=\delta_{1} q+a_{1}$ into the equation above, we have
        \begin{gather}
            y=\beta_{0}+\beta_{1} x_{1}+\cdots+\beta_{K} x_{K}+\frac{\gamma}{\delta_1}q_1 + \left( v-\frac{\gamma}{\delta_1}a_1 \right) \label{eq:5.7-2}
        \end{gather}
        In order to consistently estimate the $\beta_j(j = 0,1,2,\cdots,K)$ by an IV method in equation \eqref{eq:5.7-2}, in addition to the conditions in the question, the following condition is also required: For IV $\mathbf{z}=(1,x_1,\cdots,x_K,z_1,\cdots,z_M)$ in the reduced-form equation
        \begin{gather}
            q_{1}=\pi_{0}+\pi_{1} x_{1}+\ldots+\pi_{K} x_{K}+\theta_{1} z_{1}+\theta_2 z_2 + \cdots+\theta_{M} z_{M}+r_{1} \label{eq:5.7-3}
        \end{gather}
        at least one of $\theta_1, \theta_2, \cdots, \theta_M$ should be different from zero.
        
        \item If equation (5.45) is a $\log(wage)$ equation, $q$ is ability, $q_{1}$ is $I Q$ or some other test score, and $z_{1}, \ldots, z_{M}$ are family background variables, such as parents' education and number of siblings, describe the economic assumptions needed for consistency of the the IV procedure in part a.
        
        \textbf{Answer:} The economic meanings of the conditions in part a. are
        \begin{itemize}
            \item The family background variables are redundant in the $\log(wage)$ equation after ability and factors have been controlled for. That is, the family background variables may affect ability but should have no partial effect on $\log(wage)$ after ability and other factors have been accounted for.
            \item The family background variables should have partial effect on indicator($q_1$), after the $x_j(j=1,2,\cdots,K)$ have been controled in equation \eqref{eq:5.7-3}.
        \end{itemize}
        
        \item Carry out this procedure using the data in NLS80.RAW. Include among the explanatory variables \textit{exper, tenure, educ, married, south, urban,} and \textit{black}. First use $I Q$ as $q_{1}$ and then $K W W .$ Include in the $z_{h}$ the variables \textit{meduc, feduc, and sibs}. Discuss the results.
        
        \textbf{Answer:} Using the Stata code below, Firstly, we test the rank conditions when $q_1$ are $IQ$ and $KWW$, and the $F$ statistic for joint significance of \textit{meduc, feduc} and \textit{sibs} have p-values below 0.002, so the rank conditions have been met in a statistical sense. Secondly, we obtain the regression results of OLS and 2SLS in Table \ref{tab:5.7-c}. We can see that the return to education is small and insignificant whether $IQ$ or $KWW$ is used as the indicator(2SLS-iq and 2SLS-kww). This could be because family background variables don't satisfy the redundancy condition or they might be correlated with $a_1$.
        \input{code-5.7-c}
        \input{table-5.7-c}
    \end{enumerate}
    
    
    %problem boundary--------------------------------------------------------------------
    \item[5.8] Consider a model with unobserved heterogeneity $(q)$ and measurement error in an explanatory variable:
    \[ y=\beta_{0}+\beta_{1} x_{1}+\cdots+\beta_{K} x_{K}^{*}+q+v \]
    where $e_{K}=x_{K}-x_{K}^{*}$ is the measurement error and we set the coefficient on $q$ equal to one without loss of generality. The variable $q$ might be correlated with any of the explanatory variables, but an indicator, $q_{1}=\delta_{0}+\delta_{1} q+a_{1},$ is available. The measurement error $e_{K}$ might be correlated with the observed measure, $x_{K} .$ In addition to $q_{1},$ you also have variables $z_{1}, z_{2}, \ldots, z_{M}, M \geq 2,$ that are uncorrelated with $v, a_{1},$ and $e_{K}$.
    \begin{enumerate}
        \item Suggest an IV procedure for consistently estimating the $\beta_{j} .$ Why is $M \geq 2$ required? (Hint: Plug in $q_{1}$ for $q$ and $x_{K}$ for $x_{K}^{*}$, and go from there.)
        
        \textbf{Answer:} Plugging in $q_1$ for $q$ and $x_K$ for $x_K^*$ in the following equation: 
        \[ y=\beta_{0}+\beta_{1} x_{1}+\cdots+\beta_{K} x_{K}^{*}+q+v \]
        We have
        \[ y=\left(\beta_{0}-\frac{\delta_0}{\delta_1}\right) + \beta_{1} x_{1}+\cdots+\beta_{K} x_{K} + \frac{1}{\delta_1}q_1+\left(v-\beta_K e_K-\frac{1}{\delta_1}a_1\right) \]
        From the equation above, it is easy to know that $x_K$ and $q_1$ are the potential endogenous variables because $x_K$ and $q_1$ might be correlated with composite error $v-\beta_K e_K-\frac{1}{\delta_1}a_1$, so $M\geq 2$ is required.
        
        \item Apply this method to the model estimated in Example $5.5,$ where actual education, say educ*, plays the role of $x_{K}^{*}$. Use $I Q$ as the indicator of $q=$ \textit{ability}, and $K W W,$ \textit{meduc, feduc,} and \textit{sibs} as the elements of $\mathbf{z}$.
        
        \textbf{Answer:} Using the Stata code below, Firstly, we test the rank conditions for $x_K$ and $iq$, and the both $F$ statistic for joint significance of \textit{kww, meduc, feduc} and \textit{sibs} have p-values almost close to 0, so the rank conditions have been met in a statistical sense. Secondly, we obtain the regression results in table \ref{tab:5.8-b}. we can see that the estimated return to education is very large but not statistically significant and the coefficient of $iq$ is rarely negative but not statistically different from zero. It is seemingly that omitted $ability$ is less of problem than education measurement error in the standard $\log(wage)$ model estimated by OLS. However, insignificant coefficient for $educ$ make the evidences above not very convincing.
        \input{code-5.8-b}
        \input{table-5.8-b}
    \end{enumerate}
    
    
    %problem boundary--------------------------------------------------------------------
    \item[5.10] Consider IV estimation of the simple linear model with a single, possibly endogenous, explanatory variable, and a single instrument:
    \begin{gather*}
        y=\beta_{0}+\beta_{1} x+u, \mathrm{E}(u)=0,  \operatorname{Cov}(z, u)=0,  \operatorname{Cov}(z, x) \neq 0,  \mathrm{E}\left(u^{2} \mid z\right)=\sigma^{2}
    \end{gather*}
    \begin{enumerate}
        \item Under the preceding (standard) assumptions, show that $\avar\sqrt{N}\left(\hat{\beta}_{1}-\beta_{1}\right)$ can be expressed as $\sigma^{2} /\left(\rho_{z x}^{2} \sigma_{x}^{2}\right),$ where $\sigma_{x}^{2}=\operatorname{Var}(x)$ and $\rho_{z x}=\operatorname{Corr}(z, x) .$ Compare this result with the asymptotic variance of the OLS estimator under Assumptions OLS.1-OLS.3.
        
        \textbf{Answer:} Recall that under Assumptions 2SLS.1-2SLS.3,
        \[ \avar\sqrt{N}\left(\hat{\bm{\beta}}-\bm{\beta}\right) = \sigma^{2}\left(\left[\mathrm{E}\left(\mathbf{x}^{\prime} \mathbf{z}\right)\right]\left[\mathrm{E}\left(\mathbf{z}^{\prime} \mathbf{z}\right)\right]^{-1} \left[\mathrm{E}\left(\mathbf{z}^{\prime} \mathbf{x}\right)\right]\right)^{-1} \]
        It is easy to know that
        \[ \mathrm{E}\left(\mathbf{x}^{\prime} \mathbf{z}\right) = \begin{bmatrix}
            1 & \mathrm{E}z \\
            \mathrm{E}x & \mathrm{E}xz
        \end{bmatrix},
        \left[\mathrm{E}\left(\mathbf{z}^{\prime} \mathbf{z}\right)\right]^{-1} = \frac{1}{\var(z)} \begin{bmatrix}
            \mathrm{E}z^2 & -\mathrm{E}z \\
            -\mathrm{E}z & 1
        \end{bmatrix},
        \mathrm{E}\left(\mathbf{z}^{\prime} \mathbf{x}\right) = \begin{bmatrix}
            1 & \mathrm{E}x \\
            \mathrm{E}z & \mathrm{E}xz
        \end{bmatrix} \]
        Combine the equations above: 
        \[ \left[\mathrm{E}\left(\mathbf{x}^{\prime} \mathbf{z}\right)\right]\left[\mathrm{E}\left(\mathbf{z}^{\prime} \mathbf{z}\right)\right]^{-1} \left[\mathrm{E}\left(\mathbf{z}^{\prime} \mathbf{x}\right)\right] = \frac{1}{\var(z)} \begin{bmatrix}
            \var(z) & \mathrm{E}x \var(z) \\
            \mathrm{E}x \var(z) & \left(\mathrm{E}x\right)^2 \mathrm{E}z^2-2\mathrm{E}x\mathrm{E}z\mathrm{E}xz+\left(\mathrm{E}xz\right)^2
        \end{bmatrix}\]
        The asymptotic variance of the IV estimators will be
        \[\avar\sqrt{N}\left(\hat{\bm{\beta}}-\bm{\beta}\right) = \frac{\sigma^2}{\left[\cov(x,z)\right]^2} \begin{bmatrix}
            \left(\mathrm{E}x\right)^2 \mathrm{E}z^2-2\mathrm{E}x\mathrm{E}z\mathrm{E}xz+\left(\mathrm{E}xz\right)^2 & -\mathrm{E}x \var(z) \\
            -\mathrm{E}x \var(z) & \var(z)
        \end{bmatrix}\]
        Then
        \[\avar\sqrt{N}\left(\hat{\beta}_{1}-\beta_{1}\right) = \frac{\sigma^2\var(z)}{\left[\cov(x,z)\right]^2} = \frac{\sigma^2\sigma_z^2}{\left(\rho_{xz}\sigma_x\sigma_z\right)^2} = \frac{\sigma^2}{\rho_{xz}^2\sigma_x^2}\]
        Under Assumptions OLS.1-OLS.3, we know that the asymptotic variance for the OLS estimator is $\sigma^2/\sigma^2_x$, so the difference between the OLS and IV estimator in asymptotic variance is whether $\rho_{xz}^2$ appears in the denominator.
        
        \item Comment on how each factor affects the asymptotic variance of the IV estimator. What happens as $\rho_{z x} \rightarrow 0$ ?
        
        \textbf{Answer:} The less error variance $\sigma^2$, the more variance in $x$ and a larger correlation between $x$ and $z$ will lead to a small asymptotic variance of the IV estimator. When $\rho_{z x} \rightarrow 0$, the asymptotic variance will increases without bound. This illustrates why an instrument that is only weakly correlated with $x$ can lead to very imprecise IV estimators.
    \end{enumerate}
    
    
    %problem boundary--------------------------------------------------------------------
    \item[5.11] A model with a single endogenous explanatory variable can be written as
    \[ y_{1}=\mathbf{z}_{1} \boldsymbol{\delta}_{1}+\alpha_{1} y_{2}+u_{1}, \mathrm{E}\left(\mathbf{z}^{\prime} u_{1}\right)=\mathbf{0} \]
    where $\mathbf{z}=\left(\mathbf{z}_{1}, \mathbf{z}_{2}\right) .$ Consider the following two-step method, intended to mimic $2 \mathrm{SLS}$: (1) Regress $y_{2}$ on $\mathbf{z}_{2},$ and obtain fitted values, $\tilde{y}_{2}$(That is, $\mathbf{z}_{1}$ is omitted from the firststage regression.); (2) Regress $y_{1}$ on $\mathbf{z}_{1}, \tilde{y}_{2}$ to obtain $\tilde{\boldsymbol{\delta}}_{1}$ and $\tilde{\alpha}_{1} .$ Show that $\tilde{\boldsymbol{\delta}}_{1}$ and $\tilde{\alpha}_{1}$ are generally inconsistent. When would $\tilde{\boldsymbol{\delta}}_{1}$ and $\tilde{\alpha}_{1}$ be consistent? (Hint: Let $y_{2}^{0}$ be the population linear projection of $y_{2}$ on $\mathbf{z}_{2},$ and let $a_{2}$ be the projection error: $y_{2}=\mathbf{z}_{2} \lambda_{2}+a_{2},$ $\mathrm{E}\left(\mathbf{z}_{2}^{\prime} a_{2}\right)=\mathbf{0} .$ For simplicity, pretend that $\lambda_{2}$ is known rather than estimated; that is, assume that $\tilde{y}_{2}$ is actually $y_{2}^{0}$. Then, write
    \[ y_{1}=\mathbf{z}_{1} \boldsymbol{\delta}_{1}+\alpha_{1} y_{2}^{0}+\alpha_{1} a_{2}+u_{1} \]
    and check whether the composite error $\alpha_{1} a_{2}+u_{1}$ is uncorrelated with the explanatory variables.)
    
    \textbf{Answer:} Plug $y_2=y_2^0+a_2$ into $y_1$,
    \begin{gather}
        y_{1}=\mathbf{z}_{1} \boldsymbol{\delta}_{1}+\alpha_{1} y_{2}^0+\left(\alpha_1 a_2 + u_{1}\right) = \mathbf{z}_{1} \boldsymbol{\delta}_{1}+\alpha_{1} y_{2}^0+v_1 \label{eq:5.11-1}
    \end{gather}
    where $v_1 = \alpha_1 a_2 + u_{1}$. Due to $\mathrm{E}(\mathbf{z}_2^\prime u_1) = \mathrm{E}(\mathbf{z}_2^\prime a_2) = \mathbf{0}$, then $\mathrm{E}(\mathbf{z}_2^\prime v_1) = \mathbf{0} \implies \mathrm{E}(y_2^0 v_1) = 0$. For $\mathbf{z}_1$, although $\mathrm{E}(\mathbf{z}_1^\prime u_1) = \mathbf{0}$, in general, $\mathrm{E}(\mathbf{z}_1^\prime a_2) \neq \mathbf{0}$ because $\mathbf{z}_1$ was not included in the linear projection for $y_2$. That implies $\mathrm{E}(\mathbf{z}_1^\prime v_1) \neq \mathbf{0}$, universally making the OLS estimators($\tilde{\bm{\delta}}_1$ and $\alpha_1$) all inconsistent.
    
    
    
    
    
    
    
    
    
    




\end{enumerate}
\end{document}
